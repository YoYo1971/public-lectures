# Lecture description (work in progress)

## Summary
Despite the widespread consensus that explainable AI (XAI) is a _sine qua non_ for continous development and application of machine learning - as is exemplifed by the [EU Commission Ethics guidelines for trustworthy and AI](https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai), there is unfortunately little consensus on what interpretability in machine learning is and how to evaluate it for benchmarking. The scope of the subject matter is vast, as it should not only cover explainability from the perspective of the data scientist, but also research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations. This is all the more important given that people employ certain cognitive biases and social expectations towards the explanation process. This lecture provides an introduction to the field of explainable and trustworthy AI.

## Learning objectives
- Understand the different perspectives and 



## For discussion
- Does it make sense to include uncertainty in this lecture? I think it fits to discuss epistemic and aleatoric uncertainty, in line with discussion about explanaitions seeking to uncover causality
- 
